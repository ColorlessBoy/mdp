\relax 
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {4}Chapter4: Finite-Horizon Markov Decision Processes}{3}}
\newlabel{sec:chapter4}{{4}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}OPTIMALITY CRITERIA}{3}}
\newlabel{sub:optimality_criteria}{{4.1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Some Preliminaries}{3}}
\newlabel{subsub:some_preliminaries}{{4.1.1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}The Expected Total Reward Criterion}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Optimal Policies}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}FINITE-HORIZON POLICY EVALUATION}{4}}
\newlabel{sub:finite_horizon_policy_evaluation}{{4.2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}OPTIMALITY EQUATIONS AND THE PRINCIPLE OF OPTIMALITY}{5}}
\newlabel{sub:optimality_equations_and_the_principle_of_optimality}{{4.3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}OPTIMALITY OF DETERMINISTIC MARKOV POLICIES}{7}}
\newlabel{sub:optimality_of_deterministic_markov_policies}{{4.4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}BACKWARD INDUCTION}{8}}
\newlabel{sub:backward_induction}{{4.5}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}OPTIMALITY OF MONOTONE POLICIES}{8}}
\newlabel{sub:optimality_of_monotone_policies}{{4.6}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.1}Structured Policies}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.2}Superadditive Functions}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Optimality of Monotone Policies}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Infinite-Horizon Models: Foundations}{9}}
\newlabel{sec:infinite_horizon_models_foundations}{{5}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}THE VALUE OF A POLICY}{9}}
\newlabel{sub:the_value_of_a_policy}{{5.1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}MARKOV POLICIES}{9}}
\newlabel{sub:markov_policies}{{5.2}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discounted Markov Decision Problems}{11}}
\newlabel{sec:discounted_markov_decision_problems}{{6}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}POLECY EVALUATION (Stationary Policy)}{11}}
\newlabel{sub:polecy_evaluation}{{6.1}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}OPTIMALITY EQUATIONS}{11}}
\newlabel{sub:optimality_equations}{{6.2}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}VALUE ITERATION AND ITS VARIANTS}{14}}
\newlabel{sub:value_iteration_and_its_variants}{{6.3}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Rates of Convergence}{14}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Value Iteration Algorithm}}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Value Iteration}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}POLICY ITERATION}{16}}
\newlabel{sub:policy_iteration}{{6.4}{16}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Policy Iteration Algorithm}}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}MODIFIED POLICY ITERATION}{18}}
\newlabel{sub:modified_policy_iteration}{{6.5}{18}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Modified Policy Iteration Algorithm (MPI)}}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.1}Convergence Rates}{20}}
\newlabel{ssub:convergence_rates}{{6.5.1}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}SPANS, BOUNDS, STOPPING CRITERIA, AND RELATIVE VALUE ITEARTION}{20}}
\newlabel{sub:spans_bounds_stopping_criteria_and_relative_value_iteartion}{{6.6}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1}The Span Seminorm}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.2}Bounds on the Value of a Discounted Markov Decision Process}{22}}
\newlabel{ssub:bounds_on_the_value_of_a_discounted_markov_decision_process}{{6.6.2}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.3}Stopping Criteria}{22}}
\newlabel{ssub:stopping_criteria}{{6.6.3}{22}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Relative Value Iteration Algorithm}}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7}ACTION ELIMINATION PROCEDURES}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.7.1}Identification of Nonoptimal Actions}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.7.2}Action Elimination Procedures}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.7.3}Modified Policy Iteration with Action Elimination and an Improved Stopping Criterion}{26}}
\newlabel{ssub:modified_policy_iteration_with_action_elimination_and_an_improved_stopping_criterion}{{6.7.3}{26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.7.4}Numerical Performance of Modified Policy Iteration with Action Elimination}{26}}
\newlabel{ssub:numerical_performance_of_modified_policy_iteration_with_action_elimination}{{6.7.4}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8}CONVERGENCE OF POLICIES， TURNPIKES，AND PLANNING HORIZONS}{26}}
\newlabel{sub:convergence_of_policies_turnpikes_and_planning_horizons}{{6.8}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.9}LINEAR PROGRAMMING}{28}}
\newlabel{sub:linear_programming}{{6.9}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.9.1}Model Formulation}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.9.2}Basic Solutions and Stationary Policies}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.9.3}Optimal Solutions and Optimal Policies}{30}}
\newlabel{ssub:optimal_solutions_and_optimal_policies}{{6.9.3}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.10}Remain sections}{30}}
\newlabel{sub:remain_section}{{6.10}{30}}
