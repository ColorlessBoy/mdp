\relax 
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {4}Chapter4: Finite-Horizon Markov Decision Processes}{2}}
\newlabel{sec:chapter4}{{4}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}OPTIMALITY CRITERIA}{2}}
\newlabel{sub:optimality_criteria}{{4.1}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Some Preliminaries}{2}}
\newlabel{subsub:some_preliminaries}{{4.1.1}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}The Expected Total Reward Criterion}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Optimal Policies}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}FINITE-HORIZON POLICY EVALUATION}{3}}
\newlabel{sub:finite_horizon_policy_evaluation}{{4.2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}OPTIMALITY EQUATIONS AND THE PRINCIPLE OF OPTIMALITY}{4}}
\newlabel{sub:optimality_equations_and_the_principle_of_optimality}{{4.3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}OPTIMALITY OF DETERMINISTIC MARKOV POLICIES}{6}}
\newlabel{sub:optimality_of_deterministic_markov_policies}{{4.4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}BACKWARD INDUCTION}{7}}
\newlabel{sub:backward_induction}{{4.5}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}OPTIMALITY OF MONOTONE POLICIES}{7}}
\newlabel{sub:optimality_of_monotone_policies}{{4.6}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.1}Structured Policies}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.2}Superadditive Functions}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Optimality of Monotone Policies}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Infinite-Horizon Models: Foundations}{8}}
\newlabel{sec:infinite_horizon_models_foundations}{{5}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}THE VALUE OF A POLICY}{8}}
\newlabel{sub:the_value_of_a_policy}{{5.1}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}MARKOV POLICIES}{8}}
\newlabel{sub:markov_policies}{{5.2}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discounted Markov Decision Problems}{10}}
\newlabel{sec:discounted_markov_decision_problems}{{6}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}POLECY EVALUATION (Stationary Policy)}{10}}
\newlabel{sub:polecy_evaluation}{{6.1}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}OPTIMALITY EQUATIONS}{10}}
\newlabel{sub:optimality_equations}{{6.2}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}VALUE ITERATION AND ITS VARIANTS}{13}}
\newlabel{sub:value_iteration_and_its_variants}{{6.3}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Rates of Convergence}{13}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Value Iteration Algorithm}}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Value Iteration}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}POLICY ITERATION}{15}}
\newlabel{sub:policy_iteration}{{6.4}{15}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Policy Iteration Algorithm}}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}MODIFIED POLICY ITERATION}{17}}
\newlabel{sub:modified_policy_iteration}{{6.5}{17}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Modified Policy Iteration Algorithm (MPI)}}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.1}Convergence Rates}{19}}
\newlabel{ssub:convergence_rates}{{6.5.1}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}SPANS, BOUNDS, STOPPING CRITERIA, AND RELATIVE VALUE ITEARTION}{19}}
\newlabel{sub:spans_bounds_stopping_criteria_and_relative_value_iteartion}{{6.6}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1}The Span Seminorm}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.2}Bounds on the Value of a Discounted Markov Decision Process}{21}}
\newlabel{ssub:bounds_on_the_value_of_a_discounted_markov_decision_process}{{6.6.2}{21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.3}Stopping Criteria}{21}}
\newlabel{ssub:stopping_criteria}{{6.6.3}{21}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Relative Value Iteration Algorithm}}{22}}
