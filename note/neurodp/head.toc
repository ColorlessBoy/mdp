\contentsline {section}{\numberline {1}Introduction}{2}
\contentsline {section}{\numberline {2}Dynamic Programming}{2}
\contentsline {section}{\numberline {3}Neural Network Architectures and Training}{2}
\contentsline {section}{\numberline {4}Stochastic Iterative Algorithms}{2}
\contentsline {subsection}{\numberline {4.1}THE BASIC MODEL}{3}
\contentsline {subsection}{\numberline {4.2}CONVERGENCE BASED ON A SMOOTH POTHENTIAL FUNCTION}{3}
\contentsline {subsubsection}{\numberline {4.2.1}Convergence Proofs}{5}
\contentsline {section}{\numberline {5}Sinulation Methods for a Lookup Table Representation}{6}
\contentsline {subsection}{\numberline {5.1}SOME ASPECTS OF MONTE CARLO SIMULATION}{6}
\contentsline {subsection}{\numberline {5.2}POLICY EVALUATION BY MONTE CARLO SIMULATION}{7}
\contentsline {subsubsection}{\numberline {5.2.1}Q-Factors and Policy Iteration}{8}
\contentsline {subsection}{\numberline {5.3}TEMPORAL DIFFERENCE METHODS}{8}
\contentsline {section}{\numberline {6}Approximate DP with Cost-to-Go Function Approximation}{8}
\contentsline {subsection}{\numberline {6.1}GENERIC ISSUES-FROM PARAMETERS TO POLICIES}{8}
\contentsline {subsubsection}{\numberline {6.1.1}Generic Error Bounds}{9}
\contentsline {subsubsection}{\numberline {6.1.2}Multistage Lookahead Variations}{9}
\contentsline {subsubsection}{\numberline {6.1.3}Rollout Policies}{9}
\contentsline {subsubsection}{\numberline {6.1.4}Trading off Control Space Complexity with State Space Complexity}{9}
\contentsline {subsection}{\numberline {6.2}APPROXIMATE POLICY ITERATION}{9}
\contentsline {subsubsection}{\numberline {6.2.1}Approximate Policy Iteration Based on Monte Carlo Simulation}{10}
\contentsline {subsubsection}{\numberline {6.2.2}Error Bounds for Approximate Policy Iteration}{10}
\contentsline {subsubsection}{\numberline {6.2.3}Tightness of the Error Bounds and Empirical Behavior}{12}
\contentsline {subsection}{\numberline {6.3}APPROXIMATION POLICY EVALUATION USING USING TD$ (\lambda ) $}{14}
\contentsline {subsubsection}{\numberline {6.3.1}Approximate Policy Evaluation Using $ TD(1) $}{14}
\contentsline {subsubsection}{\numberline {6.3.2}$ TD(\lambda ) $ for General $ \lambda $}{15}
\contentsline {subsubsection}{\numberline {6.3.3}$ TD(\lambda ) $ with Linear Architectures-Discounted Problems}{17}
\contentsline {subsection}{\numberline {6.4}$ TD(\lambda ) $ with Linear Architectures --- Stochastic Shortes Path Problems}{21}
\contentsline {subsection}{\numberline {6.5}OPTIMISTIC POLICY ITERATION}{22}
