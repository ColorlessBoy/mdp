\relax 
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Dynamic Programming}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Neural Network Architectures and Training}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Stochastic Iterative Algorithms}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}THE BASIC MODEL}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}CONVERGENCE BASED ON A SMOOTH POTHENTIAL FUNCTION}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Convergence Proofs}{5}}
\newlabel{ssub:convergence_proofs}{{4.2.1}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Sinulation Methods for a Lookup Table Representation}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}SOME ASPECTS OF MONTE CARLO SIMULATION}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}POLICY EVALUATION BY MONTE CARLO SIMULATION}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Q-Factors and Policy Iteration}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}TEMPORAL DIFFERENCE METHODS}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Approximate DP with Cost-to-Go Function Approximation}{8}}
\newlabel{sec:approximate_dp_with_cost_to_go_function_approximation}{{6}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}GENERIC ISSUES-FROM PARAMETERS TO POLICIES}{8}}
\newlabel{sub:generic_issues_from_parameters_to_policies}{{6.1}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Generic Error Bounds}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Multistage Lookahead Variations}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Rollout Policies}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.4}Trading off Control Space Complexity with State Space Complexity}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}APPROXIMATE POLICY ITERATION}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Approximate Policy Iteration Based on Monte Carlo Simulation}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Error Bounds for Approximate Policy Iteration}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.3}Tightness of the Error Bounds and Empirical Behavior}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}APPROXIMATION POLICY EVALUATION USING USING TD$ (\lambda ) $}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Approximate Policy Evaluation Using $ TD(1) $}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}$ TD(\lambda ) $ for General $ \lambda $}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.3}$ TD(\lambda ) $ with Linear Architectures-Discounted Problems}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}$ TD(\lambda ) $ with Linear Architectures --- Stochastic Shortes Path Problems}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}OPTIMISTIC POLICY ITERATION}{22}}
\newlabel{sub:optimistic_policy_iteration}{{6.5}{22}}
